{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8cccdb3",
   "metadata": {},
   "source": [
    "## Next Word Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167d0e9",
   "metadata": {},
   "source": [
    "The Next Word Prediction Models are used in applications like messaging apps, search engines, virtual assistants, and autocorrect features on smartphones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57034e4c",
   "metadata": {},
   "source": [
    "Next word prediction is a language modelling task in Machine Learning that aims to predict the most probable word or sequence of words that follows a given input context. This task utilizes statistical patterns and linguistic structures to generate accurate predictions based on the context provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c9a57",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "    1. start by collecting a diverse dataset of text documents\n",
    "    2. preprocess the data by cleaning and tokenizing it\n",
    "    3. prepare the data by creating input-output pairs\n",
    "    4. engineer features such as word embeddings\n",
    "    5. select an appropriate model like an LSTM or GPT \n",
    "    6. train the model on the dataset while adjusting hyperparameters\n",
    "    7. improve the model by experimenting with different techniques and architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b5f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff6e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the text file\n",
    "with open('book/sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2901d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8200\n"
     ]
    }
   ],
   "source": [
    "#tokenize the txt to create a sequence\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26396240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input-output pairs \n",
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ddfe31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "501281df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pad the input sequence to have equal length\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f6964a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    1, 1561],\n",
       "       [   0,    0,    0, ...,    1, 1561,    5],\n",
       "       [   0,    0,    0, ..., 1561,    5,  129],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    1, 8198, 8199],\n",
       "       [   0,    0,    0, ..., 8198, 8199, 3187],\n",
       "       [   0,    0,    0, ..., 8199, 3187, 3186]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c6a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the sequence into input and output\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7162a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert output to one-hot encoding vectors\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5dd1a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'input_length': 17}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m      3\u001b[0m length \u001b[38;5;241m=\u001b[39m max_sequence_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m150\u001b[39m))\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(total_words, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages/keras/src/layers/layer.py:264\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast \u001b[38;5;241m=\u001b[39m autocast\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'input_length': 17}"
     ]
    }
   ],
   "source": [
    "#neural network architecture for model training\n",
    "model = Sequential()\n",
    "length = max_sequence_len-1\n",
    "model.add(Embedding(total_words, 100, input_length=length))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f18f33-c8d1-4d02-8b0d-cf49cf2f2342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.16.1\n",
      "Uninstalling tensorflow-2.16.1:\n",
      "  Would remove:\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/bin/import_pb_to_tensorboard\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/bin/saved_model_cli\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/bin/tensorboard\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/bin/tf_upgrade_v2\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/bin/tflite_convert\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/bin/toco\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/bin/toco_from_protos\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages/tensorflow-2.16.1.dist-info/*\n",
      "    /opt/homebrew/Cellar/jupyterlab/4.1.5/libexec/lib/python3.12/site-packages/tensorflow/*\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "pip uninstall tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a4370-851e-4dbb-b602-292ae1c57c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
